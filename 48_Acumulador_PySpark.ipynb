{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO30tu+Q/8ph5w4EPkfvZSb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luasampaio/data-engineering/blob/main/48_Acumulador_PySpark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Acumulador PySpark\n",
        "- Exemplo 01\n",
        "- Date: 2025-03-25"
      ],
      "metadata": {
        "id": "GlpNjoFvhS_W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "dt = datetime.datetime.now()\n",
        "print(dt)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nr5KMb32mexN",
        "outputId": "c9469af1-ad88-4b2d-950a-351c449612a0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-03-25 14:09:55.188462\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aVmxrxnhK2V",
        "outputId": "c4937e58-7239-49a4-8815-d6e06fa8c530"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of invalid records: 1\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "# Iniciando Sessão\n",
        "spark = SparkSession.builder.appName(\"Accumulator Example\").getOrCreate()\n",
        "# Criando acumulador\n",
        "invalid_count = spark.sparkContext.accumulator(0)\n",
        "# Example dados\n",
        "data = [\n",
        "    \"Luciana,25,M\",\n",
        "    \"Angelica,30,F\",\n",
        "    \"Helena,3,F\",\n",
        "    \"Bob,40,M\",\n",
        "    \"Alice,28,F\",\n",
        "    \"Charlie,,M\",\n",
        "\n",
        "]\n",
        "# Parallelize the data\n",
        "rdd = spark.sparkContext.parallelize(data)\n",
        "\n",
        "# Cada registro de dados será processado, e se ele não cumprir os critérios, o invalid_count é incrementado.\n",
        "def process_record(record):\n",
        "    global invalid_count\n",
        "    fields = record.split(\",\")\n",
        "    if len(fields) != 3 or not fields[1].isdigit():\n",
        "        invalid_count += 1\n",
        "        return None\n",
        "    return record\n",
        "\n",
        "# Processo RDD\n",
        "\n",
        "\n",
        "processed_rdd = rdd.map(process_record).filter(lambda x: x is not None)\n",
        "\n",
        "\n",
        "processed_rdd.collect()\n",
        "\n",
        "\n",
        "print(f\"Number of invalid records: {invalid_count.value}\")\n",
        "# Stop the Spark session\n",
        "spark.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Acumulador PySpark\n",
        "- Exemplo 02\n",
        "\n",
        "### Como contar as Linhas usando acumulador\n",
        "- O collect a ação desencadeia a transformação, fazendo com que o acumulador agregue valores das tarefas."
      ],
      "metadata": {
        "id": "nmBrgpInsV6w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "sc = SparkContext(\"local\", \"Acumulador Exemplo\")\n",
        "\n",
        "# Cria um acumulador numérico inicializado com 0\n",
        "acumulador = sc.accumulator(0)\n",
        "\n",
        "# Função para contar as linhas\n",
        "def contar_linhas(linha):\n",
        "    global acumulador\n",
        "    acumulador += 1\n",
        "    return linha\n",
        "\n",
        "dados = sc.parallelize([\"linha 1\", \"linha 2\", \"linha 3\"])\n",
        "\n",
        "# Usando foreach\n",
        "dados.foreach(contar_linhas)\n",
        "\n",
        "# Obtém o valor final do acumulador\n",
        "print(\"Número de linhas:\", acumulador.value)\n",
        "\n",
        "sc.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQ7QYNZgky5S",
        "outputId": "90933702-98d5-4b14-92e0-34595bd9412a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número de linhas: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "sc = SparkContext(\"local\", \"Acumulador Exemplo\")\n",
        "\n",
        "# Cria um acumulador numérico inicializado com 0\n",
        "acumulador = sc.accumulator(0)\n",
        "\n",
        "def contar_linhas(linha):\n",
        "    global acumulador\n",
        "    acumulador += 1\n",
        "    return linha\n",
        "\n",
        "dados = sc.parallelize([\"linha 1\", \"linha 2\", \"linha 3\"])\n",
        "dados.foreach(contar_linhas)\n",
        "\n",
        "# Obtém o valor final do acumulador\n",
        "print(\"Número de linhas:\", acumulador.value)\n",
        "\n",
        "sc.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9sUBHwLp38j",
        "outputId": "d42a2577-1e2f-473f-9cc5-43c9be052411"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número de linhas: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exemplo 03 de Acumuladores no Spark\n",
        "- verificar \"erro\" em uma linha\n"
      ],
      "metadata": {
        "id": "sXI9Q0V4tRQG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Inicia a sessão Spark\n",
        "spark = SparkSession.builder.appName(\"Exemplo03\").getOrCreate()\n",
        "\n",
        "# Cria um acumulador\n",
        "erro_count = spark.sparkContext.accumulator(0)\n",
        "\n",
        "# Função que conta linhas com erro (exemplo simples)\n",
        "def verificar_linha(linha):\n",
        "    if \"ERRO\" in linha:\n",
        "        erro_count.add(1)\n",
        "    return linha\n",
        "\n",
        "# Exemplo de RDD\n",
        "dados = [\"Linha ok\", \"ERRO: Falha no sistema\", \"Linha normal\", \"ERRO: Timeout\"]\n",
        "rdd = spark.sparkContext.parallelize(dados)\n",
        "\n",
        "# Aplica a função no RDD\n",
        "rdd.map(verificar_linha).collect()\n",
        "\n",
        "# Mostra o resultado do acumulador\n",
        "print(f\"Total de linhas com erro: {erro_count.value}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjgXmYe3tQVH",
        "outputId": "d3d887d8-968a-4868-868b-98b461b16c52"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de linhas com erro: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Exemplo 04\n"
      ],
      "metadata": {
        "id": "jg2xglA3vfVo"
      }
    },
    {
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "from pyspark.accumulators import AccumulatorParam\n",
        "\n",
        "# Inicia Spark\n",
        "spark = SparkSession.builder.appName(\"Exemplo04\").getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "# Define um acumulador de lista personalizado\n",
        "# Implement custom accumulator logic using AccumulatorParam instead of AccumulatorV2\n",
        "class ListAccumulatorParam(AccumulatorParam):\n",
        "    def zero(self, initialValue):\n",
        "        return []\n",
        "\n",
        "    def addInPlace(self, value1, value2):\n",
        "        value1.extend(value2)  # Extend the list with new values\n",
        "        return value1\n",
        "\n",
        "# Registra o acumulador using AccumulatorParam\n",
        "lista_acumulada = sc.accumulator([], ListAccumulatorParam())\n",
        "\n",
        "# Função para adicionar palavras ao acumulador\n",
        "def adicionar_palavra(palavra):\n",
        "    global lista_acumulada\n",
        "    lista_acumulada += [palavra]\n",
        "\n",
        "# Cria o RDD com palavras\n",
        "dados = sc.parallelize([\"palavra1\", \"palavra2\", \"palavra3\"])\n",
        "\n",
        "# Aplica a função para adicionar cada palavra ao acumulador\n",
        "dados.foreach(adicionar_palavra)\n",
        "\n",
        "# Exibe o resultado final\n",
        "print(\"Lista de palavras:\", lista_acumulada.value)\n",
        "\n",
        "sc.stop()"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7u-RNo1IvVj5",
        "outputId": "8a89db62-b996-45f2-daec-7101b2e0819c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lista de palavras: ['palavra1', 'palavra2', 'palavra3']\n"
          ]
        }
      ]
    }
  ]
}